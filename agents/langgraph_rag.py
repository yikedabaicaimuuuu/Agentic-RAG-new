from typing import Dict, List, Any, TypedDict, Optional
from langchain_core.documents import Document
import langgraph.graph as lg
from langgraph.graph import END, StateGraph
from functools import lru_cache
import time

from agents.reasoning_agent import ReasoningAgent
from agents.retrieval_agent import RetrievalAgent
from agents.evaluation_agent import EvaluationAgent
from agents.generation_agent import GenerationAgent

import numpy as np  # ğŸ”§ æ·»åŠ æ­¤é¡¹ç”¨äº extract_scalar

# ğŸ”§ ç”¨äºæå–å„ç±»ç»“æœä¸­çš„æ•°å€¼ï¼ˆåˆ—è¡¨/np/floatï¼‰
def extract_scalar(val):
    if isinstance(val, list) and val:
        return float(val[0])
    elif isinstance(val, (int, float, np.floating, np.generic)):
        return float(val)
    else:
        return float(val) if val is not None else 0.0


# 1) State ç±»å‹é‡Œå¯ä»¥ä¿ç•™æˆ–å¢åŠ æ›´å¤šå­—æ®µç”¨äºå¤šæŒ‡æ ‡
class AgentState(TypedDict):
    question: str
    refined_query: str
    docs: List[Document]
    answer: str
    faithfulness_score: float
    response_relevancy: float
    noise_sensitivity: float
    context_recall: float
    context_precision: float
    attempts: int
    next_step: str
    messages: List[Dict[str, Any]]
    error: Optional[str]
    start_time: float
    metrics: Dict[str, Any]
    requery_count: int
    regenerate_count: int
    max_attempts: int
    max_regenerates: int
    max_requeries: int


def create_rag_graph(
    retrieval_agent: RetrievalAgent,
    reasoning_agent: ReasoningAgent,
    generation_agent: GenerationAgent,
    evaluation_agent: EvaluationAgent
):
    # ä¿®æ”¹1: ç¡®ä¿åœ¨æŸ¥è¯¢å®Œæˆåå…³é—­å¯èƒ½æ‰“å¼€çš„æ–‡ä»¶èµ„æº
    def cached_retrieve(query: str, reference: str = None):
        try:
            result = retrieval_agent.retrieve(query, reference=reference)
            return result
        except Exception as e:
            print(f"æ£€ç´¢é”™è¯¯: {e}")
            return []

    # ä½¿ç”¨æœ‰é™å¤§å°çš„LRUç¼“å­˜ï¼Œé˜²æ­¢ç¼“å­˜è¿‡å¤§å¯¼è‡´èµ„æºè€—å°½
    retrieve_cache = {}
    max_cache_size = 20  # å‡å°ç¼“å­˜å¤§å°ï¼Œé™ä½å†…å­˜å ç”¨

    # ä¿®æ”¹2: å®ç°è‡ªå®šä¹‰ç¼“å­˜ï¼Œç¡®ä¿èµ„æºç®¡ç†
    def cached_retrieve_with_resource_mgmt(query: str, reference: str = None):
        cache_key = (query, reference)

        if cache_key in retrieve_cache:
            return retrieve_cache[cache_key]

        result = cached_retrieve(query, reference=reference)

        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—©çš„æ¡ç›®
        if len(retrieve_cache) >= max_cache_size:
            # ç§»é™¤ç¬¬ä¸€ä¸ªé”®
            oldest_key = next(iter(retrieve_cache))
            del retrieve_cache[oldest_key]

        retrieve_cache[cache_key] = result
        return result

    # -----------------------------
    # (1) æŸ¥è¯¢ä¼˜åŒ–èŠ‚ç‚¹
    # -----------------------------
    def query_optimizer(state: AgentState) -> AgentState:
        """åˆ©ç”¨ ReasoningAgent ä¼˜åŒ–ç”¨æˆ·æŸ¥è¯¢ (ä»…è¿”å› refined_query)"""
        try:
            print(f"\nğŸ§  ä¼˜åŒ–æŸ¥è¯¢: {state['question']}")
            start = time.time()

            reasoning_result = reasoning_agent.plan(
                user_question=state["question"],
                retrieved_docs=[] # è¿™é‡Œç©º; å¦‚æœéœ€è¦ä¼  docs ä¹Ÿå¯
            )
            # åŸå…ˆæ˜¯ reasoning_result["response"], ç°æ”¹ä¸º reasoning_result["refined_query"]
            refined_query = reasoning_result["refined_query"]

            duration = time.time() - start
            return {
                **state,
                "refined_query": refined_query,
                "metrics": {**state["metrics"], "query_optimization_time": duration},
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"ä¼˜åŒ–åçš„æŸ¥è¯¢: {refined_query}"
                }]
            }
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ä¼˜åŒ–å‡ºé”™: {e}")
            return {
                **state,
                "refined_query": state["question"],
                "error": f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {str(e)}",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (2) æ£€ç´¢èŠ‚ç‚¹
    # -----------------------------
    def retriever(state: AgentState) -> AgentState:
        """åŸºäº refined_query è¿›è¡Œæ£€ç´¢ï¼Œå¯é€‰åš evaluate_retrieval"""
        try:
            query = state["refined_query"]
            reference = state.get("reference", None)  # âœ… æ–°å¢
            print(f"\nğŸ“š åŸºäºä¼˜åŒ–åçš„æŸ¥è¯¢è¿›è¡Œæ£€ç´¢: {query}")

            start = time.time()
            # ä¿®æ”¹3: ä½¿ç”¨èµ„æºå®‰å…¨çš„ç¼“å­˜æ£€ç´¢å‡½æ•°
            docs = cached_retrieve_with_resource_mgmt(query, reference=reference)
            duration = time.time() - start

            if not docs:
                print("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
                return {
                    **state,
                    "docs": [],
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚",
                    "faithfulness_score": 0.0,
                    "next_step": "end",
                    "metrics": {
                        **state["metrics"],
                        "retrieval_time": duration,
                        "doc_count": 0
                    },
                    "messages": state["messages"] + [{
                        "role": "system",
                        "content": "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£"
                    }]
                }
            # âœ… é™åˆ¶ context é•¿åº¦ï¼ˆé˜²æ­¢ downstream çˆ† tokenï¼‰
            def trim_doc_text(doc):
                return doc.page_content[:3000] if len(doc.page_content) > 3000 else doc.page_content

            docs = [
                Document(page_content=trim_doc_text(doc), metadata=doc.metadata)
                for doc in docs
            ]


            # å¦‚æœæƒ³è®°å½•æ£€ç´¢è´¨é‡(Recall/Precision)ï¼Œå¯è°ƒç”¨:
            ret_eval = evaluation_agent.evaluate_retrieval(query, docs, reference=reference)
            context_precision = ret_eval.get("context_precision", 0.0)
            context_recall = ret_eval.get("context_recall", 0.0)

            print(f"ğŸ¯ Retrieval Metrics: Precision={context_precision:.2f}, Recall={context_recall:.2f}")


            return {
                **state,
                "docs": docs,
                "context_precision": context_precision,
                "context_recall": context_recall,
                "metrics": {
                    **state["metrics"],
                    "retrieval_time": duration,
                    "doc_count": len(docs),
                    "context_precision": context_precision,
                    "context_recall": context_recall
                },
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£"
                }]
            }
        except Exception as e:
            print(f"âš ï¸ æ£€ç´¢å‡ºé”™: {e}")
            return {
                **state,
                "docs": [],
                "error": f"æ£€ç´¢å¤±è´¥: {str(e)}",
                "next_step": "end",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"æ£€ç´¢å¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (3) ç”Ÿæˆç­”æ¡ˆèŠ‚ç‚¹
    # -----------------------------
    def generator(state: AgentState) -> AgentState:
        """è°ƒç”¨ GenerationAgent ç”Ÿæˆå›ç­”ï¼Œä¸åœ¨æ­¤åšè¯„ä¼°ï¼Œäº¤ç»™ evaluator èŠ‚ç‚¹åš"""
        try:
            query = state["refined_query"]
            docs = state["docs"]
            print(f"\nâœï¸ ç”Ÿæˆç­”æ¡ˆ...")

            start = time.time()
            # ç”Ÿæˆå›ç­” (å†…éƒ¨ä¼šåšå¤šæ¬¡é‡è¯•/Promptä¼˜åŒ–)
            answer_result = generation_agent.answer(query, docs, evaluation_agent)
            duration = time.time() - start

            return {
                **state,
                "answer": answer_result["answer"],
                "faithfulness_score": answer_result.get("faithfulness_score", 0.0),
                "response_relevancy": answer_result.get("response_relevancy", 0.0),
                "noise_sensitivity": answer_result.get("noise_sensitivity", 1.0),
                "metrics": {
                    **state["metrics"],
                    "generation_time": duration,
                    "cached_eval_result": answer_result.get("eval_result", None)  # âœ… æ·»åŠ ç¼“å­˜è¯„ä¼°ç»“æœ
                },
                "messages": state["messages"] + [{
                    "role": "assistant",
                    "content": answer_result["answer"]
                }]

            }
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆç­”æ¡ˆå‡ºé”™: {e}")
            return {
                **state,
                "answer": "æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚",
                "error": f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}",
                "next_step": "end",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (4) è¯„ä¼°èŠ‚ç‚¹
    # -----------------------------
    def evaluator(state: AgentState) -> AgentState:
        print(f"âš¡ Evaluator skipped (ç”± Generator å·²è¯„ä¼°)")
        return state

    # -----------------------------
    # (5) è·¯ç”±å™¨èŠ‚ç‚¹
    # -----------------------------
    def router(state: AgentState) -> AgentState:
        """
        æ ¹æ®å¤šä¸ªæŒ‡æ ‡ç»¼åˆåˆ¤æ–­ä¸‹ä¸€æ­¥:
        1) å¦‚æœæ£€ç´¢æŒ‡æ ‡ä½, ä¼˜å…ˆ re-query
        2) è‹¥æ£€ç´¢åˆæ ¼ä½†å›ç­”è´¨é‡ä¸å¤Ÿ, re-generate
        3) è‹¥å›ç­”è´¨é‡å¥½æˆ–å°è¯•æ¬¡æ•°è¾¾ä¸Šé™, end
        """

        attempts = state["attempts"]
        requery_count = state.get("requery_count", 0)
        regenerate_count = state.get("regenerate_count", 0)
        max_attempts = state.get("max_attempts", 5)
        max_requeries = state.get("max_requeries", 2)
        max_regenerates = state.get("max_regenerates", 2)



        # ---- è§£åŒ…å„æŒ‡æ ‡ ----
        # Retrieval metrics
        context_recall = state["context_recall"]    # e.g. 0.0 ~ 1.0
        context_precision = state["context_precision"]
        # Generation metrics
        faithfulness = state["faithfulness_score"]
        relevancy = state.get("response_relevancy", 0.0)
        noise = state.get("noise_sensitivity", 1.0)
        # å…¶ä»–çŠ¶æ€

        error = state.get("error", None)


        print(f"\nğŸ”„ è·¯ç”±å†³ç­–: attempts={attempts}, requery={requery_count}, regenerate={regenerate_count}")
        print(f"    â†’ recall={context_recall:.2f}, precision={context_precision:.2f}")
        print(f"    â†’ faith={faithfulness:.2f}, relevancy={relevancy:.2f}, noise={noise:.2f}")


        # è‹¥å·²å‡ºé”™, ç›´æ¥ç»“æŸ
        if error:
            return {**state, "next_step": "end", "attempts": attempts + 1}

        # --- Step 0: Generation ä¼˜å…ˆ override ---
        if faithfulness >= 0.7 and relevancy >= 0.7 and noise <= 0.4:
            print("âœ… Generation quality is high, skipping retrieval quality check. Proceed to end.")
            return {**state, "next_step": "end", "attempts": attempts + 1}

        # --- Step 1: æ£€ç´¢è´¨é‡æ˜¯å¦æ˜æ˜¾ä¸è¶³(Recall/Precisionè¿‡ä½) ---
        #    å¦‚æœç¡®å®æ£€ç´¢ä¸ç†æƒ³, æ›´å¯èƒ½éœ€è¦ re-query
            # --- æ£€ç´¢ä¸è¶³ ---
        if context_recall < 0.5 or context_precision < 0.3:
            if requery_count < max_requeries:
                return {
                    **state,
                    "next_step": "requery",
                    "requery_count": requery_count + 1,
                    "attempts": attempts + 1
                }
            else:
                # è¾¾åˆ° requery ä¸Šé™æ—¶ä¸å¼ºåˆ¶ regenerateï¼Œç›´æ¥ end
                print("âš ï¸ Retrieval attempts exhausted. Ending.")
                return {**state, "next_step": "end", "attempts": attempts + 1}

        # --- å›ç­”è´¨é‡å·® ---
        if faithfulness < 0.6 or relevancy < 0.5 or noise > 0.4:
            if regenerate_count < max_regenerates:
                return {
                    **state,
                    "next_step": "regenerate",
                    "regenerate_count": regenerate_count + 1,
                    "attempts": attempts + 1
                }
            else:
                print("âš ï¸ Generation attempts exhausted. Ending.")
                return {**state, "next_step": "end", "attempts": attempts + 1}

        # --- å›ç­”å·²è¶³å¤Ÿ ---
        return {**state, "next_step": "end", "attempts": attempts + 1}


    # -----------------------------
    # (6) requery_optimizer
    # -----------------------------
    def requery_optimizer(state: AgentState) -> AgentState:
        """åŸºäºå·²æ£€ç´¢æ–‡æ¡£å†æ¬¡è°ƒç”¨ ReasoningAgent.plan"""
        try:
            print(f"\nğŸ”„ é‡æ–°ä¼˜åŒ–æŸ¥è¯¢...")
            start = time.time()

            # è¿™æ¬¡ç»™ plan() ä¼ å…¥ docsï¼Œä»¥ä¾¿ Agent ä¼˜åŒ– query
            reasoning_result = reasoning_agent.plan(
                user_question=state["question"],
                retrieved_docs=state["docs"]
            )
            refined_query = reasoning_result["refined_query"]
            duration = time.time() - start

            return {
                **state,
                "refined_query": refined_query,
                "metrics": {
                    **state["metrics"],
                    "requery_optimization_time": duration
                },
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"é‡æ–°ä¼˜åŒ–çš„æŸ¥è¯¢: {refined_query}"
                }]
            }
        except Exception as e:
            print(f"âš ï¸ é‡æ–°ä¼˜åŒ–æŸ¥è¯¢å‡ºé”™: {e}")
            return {
                **state,
                "error": f"é‡æ–°ä¼˜åŒ–æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "next_step": "end",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"é‡æ–°ä¼˜åŒ–æŸ¥è¯¢å¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (7) finalizer
    # -----------------------------
    def finalizer(state: AgentState) -> AgentState:
        """å®Œæˆæµç¨‹, ç»Ÿè®¡æ€»æ—¶é—´"""
        total_time = time.time() - state["start_time"]
        print(f"\nâ±ï¸ æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")

        # ä¿®æ”¹4: æ¸…ç©ºç¼“å­˜ï¼Œé‡Šæ”¾èµ„æº
        retrieve_cache.clear()

        return {
            **state,
            "metrics": {**state["metrics"], "total_time": total_time}
        }

    # å»ºç«‹ StateGraph
    workflow = StateGraph(AgentState)

    # æ³¨å†ŒèŠ‚ç‚¹
    workflow.add_node("query_optimizer", query_optimizer)
    workflow.add_node("retriever", retriever)
    workflow.add_node("generator", generator)

    workflow.add_node("router", router)
    workflow.add_node("requery_optimizer", requery_optimizer)
    workflow.add_node("finalizer", finalizer)

    # è®¾ç½®èŠ‚ç‚¹é¡ºåº
    workflow.add_edge("query_optimizer", "retriever")
    workflow.add_edge("retriever", "generator")
    workflow.add_edge("generator", "router")
    workflow.add_conditional_edges(
        "router",
        lambda st: st["next_step"],
        {
            "end": "finalizer",
            "regenerate": "generator",
            "requery": "requery_optimizer"
        }
    )
    workflow.add_edge("requery_optimizer", "retriever")
    workflow.add_edge("finalizer", END)

    # è®¾ç½®å…¥å£
    workflow.set_entry_point("query_optimizer")
    return workflow.compile()

# -----------------------------
# è¿è¡Œ RAG æµç¨‹
# -----------------------------

# âœ… åœ¨ run_rag_pipeline å¼€å¤´åˆå§‹åŒ–
#evaluation_agent = EvaluationAgent(debug_mode=False)  #  debug mode

def run_rag_pipeline(
    question: str,
    retrieval_agent: RetrievalAgent,
    reasoning_agent: ReasoningAgent,
    generation_agent: GenerationAgent,
    evaluation_agent: EvaluationAgent,

    **kwargs
) -> Dict[str, Any]:

    # å¯é€‰å‚æ•°ï¼šreference ä½œä¸º ground truthï¼Œæ–¹ä¾¿è¯„ä¼°é˜¶æ®µä½¿ç”¨
    reference = kwargs.get("reference", None)

    graph = create_rag_graph(
        retrieval_agent,
        reasoning_agent,
        generation_agent,
        evaluation_agent
    )

    if kwargs.get("visualize", False):
        try:
            from IPython.display import display
            display(graph.get_graph().draw_mermaid_png())
        except Exception as e:
            print(f"æ— æ³•ç”Ÿæˆå¯è§†åŒ–: {e}")

    initial_state = {
        "question": question,
        "refined_query": "",
        "docs": [],
        "answer": "",
        "faithfulness_score": 0.0,
        "response_relevancy": 0.0,
        "noise_sensitivity": 0.0,
        "context_recall": 0.0,
        "context_precision": 0.0,
        "attempts": 0,
        "requery_count": 0,
        "regenerate_count": 0,
        "max_attempts": 5,
        "max_regenerates": 2,
        "max_requeries": 2,
        "next_step": "",
        "next_step": "",
        "error": None,
        "start_time": time.time(),
        "metrics": {},
        "messages": [{"role": "user", "content": question}],
        "reference": reference  # æ–°å¢ï¼šæ”¯æŒ reference è¯„ä¼°
    }

    result = graph.invoke(initial_state)

    print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ: {result['answer']}")
    print(f"ğŸ“Š Faithfulness: {result['faithfulness_score']:.2f}, "
          f"Relevancy: {result['response_relevancy']:.2f}, "
          f"Noise: {result['noise_sensitivity']:.2f}")

    # è¾“å‡ºæ€§èƒ½æŒ‡æ ‡
    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    for metric, value in result["metrics"].items():
        if isinstance(value, float):
            print(f"  - {metric}: {value:.2f}")
        else:
            print(f"  - {metric}: {value}")

    return result
