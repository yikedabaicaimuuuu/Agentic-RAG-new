from typing import Dict, List, Any, TypedDict, Optional
from langchain_core.documents import Document
import langgraph.graph as lg
from langgraph.graph import END, StateGraph
from functools import lru_cache
import time

from agents.reasoning_agent import ReasoningAgent
from agents.retrieval_agent import RetrievalAgent
from agents.evaluation_agent import EvaluationAgent
from agents.generation_agent import GenerationAgent

# 1) State ç±»å‹é‡Œå¯ä»¥ä¿ç•™æˆ–å¢åŠ æ›´å¤šå­—æ®µç”¨äºå¤šæŒ‡æ ‡
class AgentState(TypedDict):
    question: str
    refined_query: str
    docs: List[Document]
    answer: str
    faithfulness_score: float
    response_relevancy: float
    noise_sensitivity: float
    context_recall: float
    context_precision: float
    attempts: int
    next_step: str
    messages: List[Dict[str, Any]]
    error: Optional[str]
    start_time: float
    metrics: Dict[str, Any]

def create_rag_graph(
    retrieval_agent: RetrievalAgent,
    reasoning_agent: ReasoningAgent,
    generation_agent: GenerationAgent,
    evaluation_agent: EvaluationAgent
):
    @lru_cache(maxsize=100)
    def cached_retrieve(query: str):
        return retrieval_agent.retrieve(query)

    # -----------------------------
    # (1) æŸ¥è¯¢ä¼˜åŒ–èŠ‚ç‚¹
    # -----------------------------
    def query_optimizer(state: AgentState) -> AgentState:
        """åˆ©ç”¨ ReasoningAgent ä¼˜åŒ–ç”¨æˆ·æŸ¥è¯¢ (ä»…è¿”å› refined_query)"""
        try:
            print(f"\nğŸ§  ä¼˜åŒ–æŸ¥è¯¢: {state['question']}")
            start = time.time()

            reasoning_result = reasoning_agent.plan(
                user_question=state["question"],
                retrieved_docs=[] # è¿™é‡Œç©º; å¦‚æœéœ€è¦ä¼  docs ä¹Ÿå¯
            )
            # åŸå…ˆæ˜¯ reasoning_result["response"], ç°æ”¹ä¸º reasoning_result["refined_query"]
            refined_query = reasoning_result["refined_query"]

            duration = time.time() - start
            return {
                **state,
                "refined_query": refined_query,
                "metrics": {**state["metrics"], "query_optimization_time": duration},
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"ä¼˜åŒ–åçš„æŸ¥è¯¢: {refined_query}"
                }]
            }
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ä¼˜åŒ–å‡ºé”™: {e}")
            return {
                **state,
                "refined_query": state["question"],
                "error": f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {str(e)}",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (2) æ£€ç´¢èŠ‚ç‚¹
    # -----------------------------
    def retriever(state: AgentState) -> AgentState:
        """åŸºäº refined_query è¿›è¡Œæ£€ç´¢ï¼Œå¯é€‰åš evaluate_retrieval"""
        try:
            query = state["refined_query"]
            print(f"\nğŸ“š åŸºäºä¼˜åŒ–åçš„æŸ¥è¯¢è¿›è¡Œæ£€ç´¢: {query}")

            start = time.time()
            docs = cached_retrieve(query)
            duration = time.time() - start

            if not docs:
                print("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
                return {
                    **state,
                    "docs": [],
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚",
                    "faithfulness_score": 0.0,
                    "next_step": "end",
                    "metrics": {
                        **state["metrics"],
                        "retrieval_time": duration,
                        "doc_count": 0
                    },
                    "messages": state["messages"] + [{
                        "role": "system",
                        "content": "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£"
                    }]
                }

            # å¦‚æœæƒ³è®°å½•æ£€ç´¢è´¨é‡(Recall/Precision)ï¼Œå¯è°ƒç”¨:
            ret_eval = evaluation_agent.evaluate_retrieval(query, docs)
            context_precision = ret_eval.get("context_precision", 0.0)
            context_recall = ret_eval.get("context_recall", 0.0)

            print(f"ğŸ¯ Retrieval Metrics: Precision={context_precision:.2f}, Recall={context_recall:.2f}")


            return {
                **state,
                "docs": docs,
                "context_precision": context_precision,
                "context_recall": context_recall,
                "metrics": {
                    **state["metrics"],
                    "retrieval_time": duration,
                    "doc_count": len(docs),
                    "context_precision": context_precision,
                    "context_recall": context_recall
                },
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£"
                }]
            }
        except Exception as e:
            print(f"âš ï¸ æ£€ç´¢å‡ºé”™: {e}")
            return {
                **state,
                "docs": [],
                "error": f"æ£€ç´¢å¤±è´¥: {str(e)}",
                "next_step": "end",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"æ£€ç´¢å¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (3) ç”Ÿæˆç­”æ¡ˆèŠ‚ç‚¹
    # -----------------------------
    def generator(state: AgentState) -> AgentState:
        """è°ƒç”¨ GenerationAgent ç”Ÿæˆå›ç­”ï¼Œä¸åœ¨æ­¤åšè¯„ä¼°ï¼Œäº¤ç»™ evaluator èŠ‚ç‚¹åš"""
        try:
            query = state["refined_query"]
            docs = state["docs"]
            print(f"\nâœï¸ ç”Ÿæˆç­”æ¡ˆ...")

            start = time.time()
            # ç”Ÿæˆå›ç­” (å†…éƒ¨ä¼šåšå¤šæ¬¡é‡è¯•/Promptä¼˜åŒ–)
            answer_result = generation_agent.answer(query, docs, evaluation_agent)
            duration = time.time() - start

            return {
                **state,
                "answer": answer_result["answer"],
                "faithfulness_score": answer_result.get("faithfulness_score", 0.0),
                "response_relevancy": answer_result.get("response_relevancy", 0.0),
                "noise_sensitivity": answer_result.get("noise_sensitivity", 1.0),
                "metrics": {**state["metrics"], "generation_time": duration},
                "messages": state["messages"] + [{
                    "role": "assistant",
                    "content": answer_result["answer"]
                }]
            }
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆç­”æ¡ˆå‡ºé”™: {e}")
            return {
                **state,
                "answer": "æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚",
                "error": f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}",
                "next_step": "end",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (4) è¯„ä¼°èŠ‚ç‚¹
    # -----------------------------
    def evaluator(state: AgentState) -> AgentState:
        """
        è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆè´¨é‡:
        - ä¸å†ç”¨ full_evaluate, æ”¹ç”¨ evaluate_generation
        - å– faithfulness, response_relevancy, noise_sensitivity
        """
        try:
            query = state["refined_query"]
            docs = state["docs"]
            answer = state["answer"]

            print(f"\nğŸ“Š è¯„ä¼°ç­”æ¡ˆè´¨é‡...")
            start = time.time()
            # ç”¨â€œå›ç­”è¯„ä¼°â€
            eval_result = evaluation_agent.evaluate_generation(query, docs, answer)
            duration = time.time() - start

            faithfulness = float(eval_result.get("faithfulness", 0))
            relevancy = float(eval_result.get("response_relevancy", 0))
            noise_keys = [k for k in eval_result.keys() if k.startswith("noise_sensitivity")]
            noise = evaluation_agent._get_numeric_value(eval_result[noise_keys[0]]) if noise_keys else 0.0


            return {
                **state,
                "faithfulness_score": faithfulness,
                "response_relevancy": relevancy,
                "noise_sensitivity": noise,
                "metrics": {**state["metrics"], "evaluation_time": duration},
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": (
                        f"è¯„ä¼°ç»“æœ: å¿ å®åº¦={faithfulness:.2f}, "
                        f"ç›¸å…³åº¦={relevancy:.2f}, å™ªéŸ³æ•æ„Ÿåº¦={noise:.2f}"
                    )
                }]
            }
        except Exception as e:
            print(f"âš ï¸ è¯„ä¼°å‡ºé”™: {e}")
            return {
                **state,
                "faithfulness_score": 0.0,
                "response_relevancy": 0.0,
                "noise_sensitivity": 1.0,
                "error": f"è¯„ä¼°å¤±è´¥: {str(e)}",
                "next_step": "end",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"è¯„ä¼°å¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (5) è·¯ç”±å™¨èŠ‚ç‚¹
    # -----------------------------
    def router(state: AgentState) -> AgentState:
        """
        æ ¹æ®å¤šä¸ªæŒ‡æ ‡ç»¼åˆåˆ¤æ–­ä¸‹ä¸€æ­¥:
        1) å¦‚æœæ£€ç´¢æŒ‡æ ‡ä½, ä¼˜å…ˆ re-query
        2) è‹¥æ£€ç´¢åˆæ ¼ä½†å›ç­”è´¨é‡ä¸å¤Ÿ, re-generate
        3) è‹¥å›ç­”è´¨é‡å¥½æˆ–å°è¯•æ¬¡æ•°è¾¾ä¸Šé™, end
        """

        # ---- è§£åŒ…å„æŒ‡æ ‡ ----
        # Retrieval metrics
        context_recall = state["context_recall"]    # e.g. 0.0 ~ 1.0
        context_precision = state["context_precision"]
        # Generation metrics
        faithfulness = state["faithfulness_score"]
        relevancy = state.get("response_relevancy", 0.0)
        noise = state.get("noise_sensitivity", 1.0)
        # å…¶ä»–çŠ¶æ€
        attempts = state["attempts"]
        error = state.get("error", None)

        print(f"\nğŸ”„ è·¯ç”±å†³ç­–: attempt={attempts}, "
            f"recall={context_recall:.2f}, prec={context_precision:.2f}, "
            f"faith={faithfulness:.2f}, relevancy={relevancy:.2f}, noise={noise:.2f}")

        # è‹¥å·²å‡ºé”™, ç›´æ¥ç»“æŸ
        if error:
            return {**state, "next_step": "end", "attempts": attempts + 1}

        # --- Step 1: æ£€ç´¢è´¨é‡æ˜¯å¦æ˜æ˜¾ä¸è¶³(Recall/Precisionè¿‡ä½) ---
        #    å¦‚æœç¡®å®æ£€ç´¢ä¸ç†æƒ³, æ›´å¯èƒ½éœ€è¦ re-query
        if context_recall < 0.5 or context_precision < 0.3:
            if attempts >= 3:
                # å¤šæ¬¡å°è¯•åä»ä¸è¡Œ => æ”¾å¼ƒ
                next_step = "end"
            else:
                next_step = "requery"
            return {**state, "attempts": attempts + 1, "next_step": next_step}

        # --- Step 2: å›ç­”è´¨é‡æ£€æŸ¥ (Faithfulness, Relevancy, Noiseç­‰) ---
        #   2.1 Faithfulness (å›ç­”æ˜¯å¦å¿ å®ä¸Šä¸‹æ–‡)
        #       è‹¥ä½äºé˜ˆå€¼(0.6), å†æ¬¡ç”Ÿæˆ(æˆ–å†³å®šæ”¹å†™ Query?)
        if faithfulness < 0.6:
            # å¯ä»¥åˆ¤æ–­ attempts æ¬¡æ•°, å†³å®šæ˜¯ re-generate è¿˜æ˜¯ re-query
            if attempts < 2:
                next_step = "regenerate"
            else:
                next_step = "requery"
            return {**state, "attempts": attempts + 1, "next_step": next_step}

        #   2.2 Relevancy (å›ç­”ä¸é—®é¢˜å¯¹é½ç¨‹åº¦)
        #       è‹¥ç›¸å…³åº¦è¿‡ä½(<0.5), å¯èƒ½åªéœ€ re-generate
        if relevancy < 0.5:
            if attempts < 3:
                next_step = "regenerate"
            else:
                next_step = "end"
            return {**state, "attempts": attempts + 1, "next_step": next_step}

        #   2.3 NoiseSensitivity (å›ç­”æ˜¯å¦è¢«é”™è¯¯ä¿¡æ¯å¹²æ‰°)
        #       è‹¥å™ªéŸ³æ•æ„Ÿåº¦è¿‡é«˜(>0.4), ä¹Ÿå¯ re-generate
        if noise > 0.4:
            if attempts < 2:
                next_step = "regenerate"
            else:
                # å¦‚æœå¤šæ¬¡éƒ½æ”¹ä¸æ‰å¹²æ‰° => requery
                next_step = "requery"
            return {**state, "attempts": attempts + 1, "next_step": next_step}

        # --- Step 3: å…¨éƒ¨æŒ‡æ ‡å°šå¯, or å·²è¾¾å°è¯•ä¸Šé™ => end ---
        if attempts >= 3:
            next_step = "end"
        else:
            next_step = "end"  # å›ç­”å·²è¾¾æ ‡, æ‰€ä»¥ç»“æŸ

        return {**state, "attempts": attempts + 1, "next_step": next_step}


    # -----------------------------
    # (6) requery_optimizer
    # -----------------------------
    def requery_optimizer(state: AgentState) -> AgentState:
        """åŸºäºå·²æ£€ç´¢æ–‡æ¡£å†æ¬¡è°ƒç”¨ ReasoningAgent.plan"""
        try:
            print(f"\nğŸ”„ é‡æ–°ä¼˜åŒ–æŸ¥è¯¢...")
            start = time.time()

            # è¿™æ¬¡ç»™ plan() ä¼ å…¥ docsï¼Œä»¥ä¾¿ Agent ä¼˜åŒ– query
            reasoning_result = reasoning_agent.plan(
                user_question=state["question"],
                retrieved_docs=state["docs"]
            )
            refined_query = reasoning_result["refined_query"]
            duration = time.time() - start

            return {
                **state,
                "refined_query": refined_query,
                "metrics": {
                    **state["metrics"],
                    "requery_optimization_time": duration
                },
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"é‡æ–°ä¼˜åŒ–çš„æŸ¥è¯¢: {refined_query}"
                }]
            }
        except Exception as e:
            print(f"âš ï¸ é‡æ–°ä¼˜åŒ–æŸ¥è¯¢å‡ºé”™: {e}")
            return {
                **state,
                "error": f"é‡æ–°ä¼˜åŒ–æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "next_step": "end",
                "messages": state["messages"] + [{
                    "role": "system",
                    "content": f"é‡æ–°ä¼˜åŒ–æŸ¥è¯¢å¤±è´¥: {str(e)}"
                }]
            }

    # -----------------------------
    # (7) finalizer
    # -----------------------------
    def finalizer(state: AgentState) -> AgentState:
        """å®Œæˆæµç¨‹, ç»Ÿè®¡æ€»æ—¶é—´"""
        total_time = time.time() - state["start_time"]
        print(f"\nâ±ï¸ æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")

        return {
            **state,
            "metrics": {**state["metrics"], "total_time": total_time}
        }

    # å»ºç«‹ StateGraph
    workflow = StateGraph(AgentState)

    # æ³¨å†ŒèŠ‚ç‚¹
    workflow.add_node("query_optimizer", query_optimizer)
    workflow.add_node("retriever", retriever)
    workflow.add_node("generator", generator)
    workflow.add_node("evaluator", evaluator)
    workflow.add_node("router", router)
    workflow.add_node("requery_optimizer", requery_optimizer)
    workflow.add_node("finalizer", finalizer)

    # è®¾ç½®èŠ‚ç‚¹é¡ºåº
    workflow.add_edge("query_optimizer", "retriever")
    workflow.add_edge("retriever", "generator")
    workflow.add_edge("generator", "evaluator")
    workflow.add_edge("evaluator", "router")
    workflow.add_conditional_edges(
        "router",
        lambda st: st["next_step"],
        {
            "end": "finalizer",
            "regenerate": "generator",
            "requery": "requery_optimizer"
        }
    )
    workflow.add_edge("requery_optimizer", "retriever")
    workflow.add_edge("finalizer", END)

    # è®¾ç½®å…¥å£
    workflow.set_entry_point("query_optimizer")
    return workflow.compile()

# -----------------------------
# è¿è¡Œ RAG æµç¨‹
# -----------------------------
def run_rag_pipeline(
    question: str,
    retrieval_agent: RetrievalAgent,
    reasoning_agent: ReasoningAgent,
    generation_agent: GenerationAgent,
    evaluation_agent: EvaluationAgent,
    **kwargs
) -> Dict[str, Any]:

    graph = create_rag_graph(
        retrieval_agent,
        reasoning_agent,
        generation_agent,
        evaluation_agent
    )

    if kwargs.get("visualize", False):
        try:
            from IPython.display import display
            display(graph.get_graph().draw_mermaid_png())
        except Exception as e:
            print(f"æ— æ³•ç”Ÿæˆå¯è§†åŒ–: {e}")

    initial_state = {
        "question": question,
        "refined_query": "",
        "docs": [],
        "answer": "",
        "faithfulness_score": 0.0,
        "response_relevancy": 0.0,
        "noise_sensitivity": 0.0,
        "context_recall": 0.0,
        "context_precision": 0.0,
        "attempts": 0,
        "next_step": "",
        "error": None,
        "start_time": time.time(),
        "metrics": {},
        "messages": [{"role": "user", "content": question}]
    }

    result = graph.invoke(initial_state)

    print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ: {result['answer']}")
    print(f"ğŸ“Š Faithfulness: {result['faithfulness_score']:.2f}, "
          f"Relevancy: {result['response_relevancy']:.2f}, "
          f"Noise: {result['noise_sensitivity']:.2f}")

    # è¾“å‡ºæ€§èƒ½æŒ‡æ ‡
    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    for metric, value in result["metrics"].items():
        if isinstance(value, float):
            print(f"  - {metric}: {value:.2f}")
        else:
            print(f"  - {metric}: {value}")

    return result
