from langchain_openai import ChatOpenAI
from ragas import EvaluationDataset, evaluate
from langchain.embeddings import OpenAIEmbeddings

from ragas.metrics import (
    ContextPrecision,
    LLMContextRecall,
    Faithfulness,
    ResponseRelevancy,
    NoiseSensitivity
)
from ragas.llms import LangchainLLMWrapper
import traceback

class EvaluationAgent:
    """
    å¤šåŠŸèƒ½è¯„ä¼°Agent:
      1) quick_evaluate(...) ä»…ç”¨LLMç®€å•åˆ¤æ–­æ£€ç´¢æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜
      2) evaluate_retrieval(...) ä¸“é—¨è®¡ç®—æ£€ç´¢Precision/Recall
      3) evaluate_generation(...) ä¸“é—¨è®¡ç®—å›ç­”è´¨é‡(Faithfulnessç­‰)
      4) full_evaluate(...) å…¼å®¹ä»¥å‰çš„ç»¼åˆè¯„ä¼°é€»è¾‘(åŒæ—¶ç®— context_precision, context_recall, faithfulness)
    """

    def __init__(self, model_name="gpt-3.5-turbo", embeddings=None):
        self.llm = ChatOpenAI(model_name=model_name, temperature=0)
        if embeddings is None:
            try:
                # Try to import and initialize embeddings (OpenAI embeddings or other)
                print("ğŸ“¦ Initializing OpenAI embeddings for ResponseRelevancy")
                self.embeddings = OpenAIEmbeddings()
            except (ImportError, Exception) as e:
                print(f"âš ï¸ Could not initialize embeddings: {str(e)}")
                print("âš ï¸ ResponseRelevancy will have reduced effectiveness")
                self.embeddings = None
        else:
            self.embeddings = embeddings  # Use provided embeddings

        # åŸå…ˆ full_evaluate ä½¿ç”¨çš„é€šç”¨ metrics
        # (ContextPrecision, LLMContextRecall, Faithfulness)
        self.metrics = [
            ContextPrecision(),
            LLMContextRecall(),
            Faithfulness()
        ]

        # å¯é€‰: å¦‚æœæƒ³ä¸“é—¨åŒºåˆ†retrieval vs generationï¼Œç”¨ä¸åŒåˆ—è¡¨:
        # self.retrieval_metrics = [...]
        # self.generation_metrics = [...]

    # ----------------------------------------------------------------------
    # 1) quick_evaluate(...) - ä¿ç•™åŸé€»è¾‘
    # ----------------------------------------------------------------------
    def quick_evaluate(self, question, docs):
        """
        ä»…ä½¿ç”¨ LLM è¯„ä¼°ï¼šåˆ¤æ–­æ£€ç´¢æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œå¹¶æä¾›å…³é”®è¯å»ºè®®ã€‚
        """
        if not docs:
            return {"sufficient": False, "suggested_keywords": "expand keywords"}

        context = " ".join([doc.page_content for doc in docs])
        eval_prompt = (
            f"Question: {question}\n"
            f"Retrieved content: {context[:1000]}...\n"
            "Is this information sufficient to answer the question? "
            "Please respond with 'sufficient' or 'insufficient' and provide additional keywords."
        )

        eval_response = self.llm.predict(eval_prompt)
        sufficient = "sufficient" in eval_response.lower()
        suggested_keywords = " ".join(eval_response.split()[-3:])

        return {"sufficient": sufficient, "suggested_keywords": suggested_keywords}

    # ----------------------------------------------------------------------
    # 2) æå–æ•°å­—çš„å·¥å…·å‡½æ•° - ä¿ç•™åŸé€»è¾‘
    # ----------------------------------------------------------------------
    def _get_numeric_value(self, value):
        """
        Extract a numeric value from various possible data types:
        - If it's a list, return the average
        - If it's a number, return it directly
        - Otherwise return 0
        """
        # Handle None case
        if value is None:
            return 0

        # Handle direct numeric case
        if isinstance(value, (int, float)):
            return value

        # Handle list case - take average of numeric values
        elif isinstance(value, list):
            if not value:
                return 0
            numeric_values = [v for v in value if isinstance(v, (int, float))]
            if not numeric_values:
                return 0
            return sum(numeric_values) / len(numeric_values)

        # Try to convert to float
        try:
            return float(value)
        except (TypeError, ValueError):
            # Try to extract from object if it has a numeric attribute
            if hasattr(value, "value") and isinstance(getattr(value, "value"), (int, float)):
                return getattr(value, "value")
            return 0

    def _extract_score(self, result, metric_name):
        """
        Extract score from RAGAS result structure, handling different formats
        """
        # If result is a dictionary (most common case)
        if isinstance(result, dict):
            if metric_name in result:
                return result[metric_name]

        # If result is an EvaluationResult object with a dict-like interface
        if hasattr(result, "__getitem__") and not isinstance(result, str):
            try:
                return result[metric_name]
            except (KeyError, TypeError):
                pass

        # If result has scores attribute (some versions of RAGAS use this)
        if hasattr(result, "scores"):
            scores = result.scores
            if isinstance(scores, dict) and metric_name in scores:
                return scores[metric_name]

        # If result has a dict data attribute
        if hasattr(result, "data") and isinstance(result.data, dict):
            if metric_name in result.data:
                return result.data[metric_name]

        # For list-like structures with name attributes
        if hasattr(result, "__iter__") and not isinstance(result, (str, dict)):
            for item in result:
                if hasattr(item, "name") and item.name == metric_name:
                    return item.score

        # Get the type and available attributes/keys for debugging
        result_type = type(result).__name__
        available_attrs = []

        # Check for dict keys
        if isinstance(result, dict):
            available_attrs = list(result.keys())
        # Check for object attributes
        elif hasattr(result, "__dict__"):
            available_attrs = list(result.__dict__.keys())
        # Check for scores attribute
        elif hasattr(result, "scores"):
            if isinstance(result.scores, dict):
                available_attrs = list(result.scores.keys())

        print(f"âš ï¸ Could not find metric '{metric_name}' in result structure of type '{result_type}'")
        print(f"âš ï¸ Available attributes/keys: {available_attrs}")

        return 0

    # ----------------------------------------------------------------------
    # 3) evaluate_retrieval(...) - æ–°å¢: åªè¯„ä¼°æ£€ç´¢è´¨é‡(Precision/Recall)
    # ----------------------------------------------------------------------
    def evaluate_retrieval(self, user_query, retrieved_docs, reference=None):
        """
        Specialized function for RetrieverAgent:
        - Only evaluates context_precision, context_recall
        """
        retrieved_texts = [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]
        if not retrieved_texts:
            print("âš ï¸ Warning: No retrieved texts to evaluate in retrieval!")
            retrieved_texts = ["N/A"]

        # Build RAGAS data
        data = {
            "user_input": user_query,
            "retrieved_contexts": retrieved_texts,
            "response": "N/A",  # No response text at retrieval stage
            "reference": reference if reference else "N/A"
        }

        print(f"ğŸ” DEBUG: Query length: {len(user_query)}")
        print(f"ğŸ” DEBUG: Retrieved docs count: {len(retrieved_texts)}")

        dataset = EvaluationDataset.from_list([data])

        # Metrics for Precision/Recall
        metrics = [ContextPrecision(), LLMContextRecall()]

        try:
            result = evaluate(dataset=dataset, metrics=metrics, llm=LangchainLLMWrapper(self.llm))

            print("\nğŸ” Debugging retrieval evaluation result type:", type(result))
            print("ğŸ” Debugging retrieval evaluation result:", result)

            # DIRECT ACCESS - this is the key fix based on the debug output you provided
            if isinstance(result, dict):
                context_precision = self._get_numeric_value(result.get('context_precision', 0))
                context_recall = self._get_numeric_value(result.get('context_recall', 0))
            else:
                # Fall back to the extraction method if not a dict
                context_precision = self._get_numeric_value(self._extract_score(result, "context_precision"))
                context_recall = self._get_numeric_value(self._extract_score(result, "context_recall"))

            print(f"ğŸ¯ Context Precision: {context_precision:.4f}")
            print(f"ğŸ“ˆ Context Recall: {context_recall:.4f}")

        except Exception as e:
            print(f"âŒ Error in evaluate_retrieval: {str(e)}")
            import traceback
            traceback.print_exc()
            context_precision = 0
            context_recall = 0

        return {
            "context_precision": context_precision,
            "context_recall": context_recall
        }
    # ----------------------------------------------------------------------
    # 4) evaluate_generation(...) - æ–°å¢: åªè¯„ä¼°å›ç­”è´¨é‡(Faithfulnessç­‰)
    # ----------------------------------------------------------------------
    def evaluate_generation(self, user_query, retrieved_docs, response, reference=None):
        """
        Evaluates response quality:
        - Faithfulness (whether faithful to retrieved_docs)
        - ResponseRelevancy (whether response aligns with user_query)
        - NoiseSensitivity (whether response is disturbed by incorrect context)
        """

        retrieved_texts = [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]
        if not retrieved_texts:
            print("âš ï¸ Warning: No retrieved texts to evaluate in generation!")
            retrieved_texts = ["N/A"]

        if not response or response.strip() == "":
            print("âš ï¸ Warning: Empty response to evaluate!")
            response = "N/A"

        data = {
            "user_input": user_query,
            "retrieved_contexts": retrieved_texts,
            "response": response,
            "reference": reference if reference else "N/A"
        }

        print(f"ğŸ” DEBUG: Query length: {len(user_query)}")
        print(f"ğŸ” DEBUG: Retrieved docs: {len(retrieved_texts)}")
        print(f"ğŸ” DEBUG: Response length: {len(response)}")

        dataset = EvaluationDataset.from_list([data])

        try:
            # Check if embeddings are available for ResponseRelevancy
            if self.embeddings is None:
                print("âš ï¸ Warning: No embeddings provided for ResponseRelevancy - using simplified metrics")
                metrics = [Faithfulness(), NoiseSensitivity(llm=LangchainLLMWrapper(self.llm))]
            else:
                metrics = [
                    Faithfulness(),
                    ResponseRelevancy(embeddings=self.embeddings, llm=LangchainLLMWrapper(self.llm)),
                    NoiseSensitivity(llm=LangchainLLMWrapper(self.llm))
                ]

            result = evaluate(dataset=dataset, metrics=metrics, llm=LangchainLLMWrapper(self.llm))

            print(f"\nğŸ” Debugging: Result type: {type(result).__name__}")
            print(f"ğŸ” Debugging generation evaluation result: {result}")

            # Extract the dictionary data from the result
            result_dict = {}

            # If it's already a dictionary
            if isinstance(result, dict):
                result_dict = result
            # Try to access through __dict__ attribute
            elif hasattr(result, '__dict__'):
                # Try to extract scores from internal attributes
                if '_scores_dict' in result.__dict__:
                    result_dict = result._scores_dict
                elif 'scores' in result.__dict__ and isinstance(result.scores, dict):
                    result_dict = result.scores
            # If it has a string representation that looks like a dict, try to parse it
            else:
                try:
                    import ast
                    str_result = str(result)
                    if str_result.startswith('{') and str_result.endswith('}'):
                        result_dict = ast.literal_eval(str_result)
                except:
                    pass

            # Print the extracted dictionary
            print(f"ğŸ” DEBUG: Extracted result dict: {result_dict}")
            print(f"ğŸ” DEBUG: Result dict keys: {list(result_dict.keys())}")

            # Get metrics from the extracted dictionary
            faithfulness = self._get_numeric_value(result_dict.get('faithfulness', 0))

            # Check for either 'answer_relevancy' or 'response_relevancy'
            relevancy = 0
            if 'answer_relevancy' in result_dict:
                relevancy = self._get_numeric_value(result_dict.get('answer_relevancy'))
            elif 'response_relevancy' in result_dict:
                relevancy = self._get_numeric_value(result_dict.get('response_relevancy'))

            # Check for noise_sensitivity with any additional suffix
            noise_sensitivity = 0
            for key in result_dict.keys():
                if key.startswith('noise_sensitivity'):
                    noise_sensitivity = self._get_numeric_value(result_dict.get(key))
                break

            print(f"ğŸ“Š Faithfulness: {faithfulness:.4f}")
            print(f"ğŸ¯ Response Relevancy: {relevancy:.4f}")
            print(f"ğŸ”Š Noise Sensitivity: {noise_sensitivity:.4f}")

        except Exception as e:
            print(f"âŒ Error in evaluate_generation: {str(e)}")
            import traceback
            traceback.print_exc()
            faithfulness = 0
            relevancy = 0
            noise_sensitivity = 0

        return {
            "faithfulness": faithfulness,
            "response_relevancy": relevancy,
            "noise_sensitivity": noise_sensitivity
        }

    # ----------------------------------------------------------------------
    # 5) full_evaluate(...) - ä¿ç•™å…¼å®¹æ—§é€»è¾‘
    # ----------------------------------------------------------------------
    def full_evaluate(self, query, retrieved_docs, response=None, reference=None):
        """
        ä½¿ç”¨ RAGAS è¿›è¡Œå®Œæ•´è¯„ä¼°ï¼š
        - æ£€ç´¢è´¨é‡ï¼ˆcontext precision, recallï¼‰
        - ç”Ÿæˆè´¨é‡ï¼ˆfaithfulnessï¼‰
        """
        retrieved_texts = [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]
        if not retrieved_texts:
            print("âš ï¸ Warning: No retrieved texts to evaluate!")
            retrieved_texts = ["N/A"]

        # æå– ground truth reference

        data = {
            "user_input": query,
            "retrieved_contexts": retrieved_texts,
            "response": response if response else "N/A",
            "reference": reference if reference else "N/A"
        }
        # Print debug info
        print(f"\nğŸ” DEBUG: Query length: {len(query)}")
        print(f"ğŸ” DEBUG: Retrieved docs: {len(retrieved_texts)}")
        print(f"ğŸ” DEBUG: Response length: {len(response) if response else 0}")

        dataset = EvaluationDataset.from_list([data])
        metrics = [ContextPrecision(), LLMContextRecall(), Faithfulness()]
        try:
            result = evaluate(
                dataset=dataset,
                metrics=metrics,
                llm=LangchainLLMWrapper(self.llm)
            )

            print("\nğŸ” Debugging: Raw Evaluation Result â", result)

            # å…¼å®¹ä¸åŒè¿”å›ç»“æ„ï¼šragas 1.x ä½¿ç”¨ result.scoresï¼Œå¦åˆ™ç›´æ¥ç”¨ result æœ¬èº«
            scores = getattr(result, "scores", result)

            if not scores:
                print("âŒ Scores object is empty or None")
                return {"faithfulness": 0, "context_recall": 0, "context_precision": 0}

            # Print all available keys for debugging
            print(f"ğŸ”‘ Available score keys: {list(scores.keys()) if hasattr(scores, 'keys') else 'No keys'}")

            faithfulness_score = self._get_numeric_value(scores.get("faithfulness", 0))
            context_recall = self._get_numeric_value(scores.get("context_recall", 0))
            context_precision = self._get_numeric_value(scores.get("context_precision", 0))

            print("âœ… Metrics extracted successfully")

        except Exception as e:
            print(f"âŒ Detailed error in evaluation: {str(e)}")

            traceback.print_exc()
            faithfulness_score = 0
            context_recall = 0
            context_precision = 0

        print(f"ğŸ“Š Faithfulness Score: {faithfulness_score:.4f}")
        print(f"ğŸ“ˆ Context Recall: {context_recall:.4f}")
        print(f"ğŸ¯ Context Precision: {context_precision:.4f}")

        return {
            "faithfulness": faithfulness_score,
            "context_recall": context_recall,
            "context_precision": context_precision
        }
